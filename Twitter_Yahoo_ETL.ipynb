{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: From Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime as dt\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import datetime as dt\n",
    "from searchtweets import ResultStream, gen_rule_payload, load_credentials, collect_results\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "pymysql.install_as_MySQLdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the date of week ago, since our standard user package only allows 7 days of tweets access\n",
    "last_week=(dt.datetime.now() - dt.timedelta(days=7)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which date of the last 7 days would you like to end?  YYYY-MM-DD  2019-04-16\n"
     ]
    }
   ],
   "source": [
    "# Ask for a date in the past 7 days and the app below will base on the date entered\"\n",
    "end_date=input(\"Which date of the last 7 days would you like to end?  YYYY-MM-DD  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Standard Account, you have chosen the time frame from 2019-04-10 to 2019-04-16.\n",
      "Update to Premium Account to see more.\n"
     ]
    }
   ],
   "source": [
    "# Display time frame\n",
    "print(f'With Standard Account, you have chosen the time frame from {last_week} to {end_date}.')\n",
    "print(\"Update to Premium Account to see more.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up args and load API keys into it to generate bearer_token\n",
    "premium_search_args = load_credentials(\"twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_premium\",\n",
    "                                       env_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up token and headers for requests.get\n",
    "premium_search_args.get('bearer_token')\n",
    "headers={'Authorization': 'Bearer ' + premium_search_args.get('bearer_token')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a list of keywords, which can be enriched in the future\n",
    "keyword_list=[\"nextdaytrade\",\"stocktwits\",\"stockwatch\",\"stockmarket\",\n",
    "              \"stocktrend\",'biostocks','biostock','stockstream','daystock',\n",
    "              'stocklist','stockwizard','toptraders','toptrader','topstock',\n",
    "             'wallstreet','newyorkstock','nyse','bloomberg','newyorktimes',\n",
    "             'protrader','protraders','stockpick','stockfinance','stockpicks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple for loops to extract tickers from tweets allowed to access and use Regex to search only tickers containing $ sign\n",
    "extracted_tickers=[]\n",
    "for keyword in keyword_list:\n",
    "    url='https://api.twitter.com/1.1/search/tweets.json?q='+keyword+'&until='+end_date+\"lang=en\"\n",
    "    rjson = requests.get(url,headers=headers).json()\n",
    "    \n",
    "    for i in range(len(rjson['statuses'])):\n",
    "        extracted_text = rjson['statuses'][i]['text']\n",
    "        extracted_list=re.findall(r'\\$\\b[A-Z]{2,4}\\b[.!?]?',extracted_text)\n",
    "        looplist=[]\n",
    "        for t in extracted_list:\n",
    "            looplist.append(t[1:])\n",
    "        extracted_tickers=extracted_tickers+looplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PCG',\n",
       " 'DIS',\n",
       " 'CGC',\n",
       " 'BA',\n",
       " 'GS',\n",
       " 'NFLX',\n",
       " 'UNH',\n",
       " 'ROKU',\n",
       " 'AAPL',\n",
       " 'TSLA',\n",
       " 'LYFT',\n",
       " 'BA',\n",
       " 'UNH',\n",
       " 'TSLA',\n",
       " 'BBBY',\n",
       " 'CGC',\n",
       " 'AAPL',\n",
       " 'LYFT',\n",
       " 'LYFT',\n",
       " 'BBBY',\n",
       " 'AAPL',\n",
       " 'TSLA',\n",
       " 'ROKU',\n",
       " 'NVDA',\n",
       " 'MNST',\n",
       " 'BA',\n",
       " 'BIDU',\n",
       " 'AAPL',\n",
       " 'TSLA',\n",
       " 'ROKU',\n",
       " 'NBEV',\n",
       " 'AAPL',\n",
       " 'BA',\n",
       " 'ROKU',\n",
       " 'TSLA',\n",
       " 'CF',\n",
       " 'XLP',\n",
       " 'BB',\n",
       " 'TSLA',\n",
       " 'VZ',\n",
       " 'TER',\n",
       " 'PREV',\n",
       " 'CCBG',\n",
       " 'WWE',\n",
       " 'ADNT',\n",
       " 'BOOM',\n",
       " 'PCG',\n",
       " 'CYBR',\n",
       " 'CVNA',\n",
       " 'YETI',\n",
       " 'PUMP',\n",
       " 'RDN',\n",
       " 'NOK',\n",
       " 'VALE',\n",
       " 'NIO',\n",
       " 'ABEV',\n",
       " 'XHR',\n",
       " 'FCSC',\n",
       " 'XON',\n",
       " 'BHVN',\n",
       " 'ADVM',\n",
       " 'ACST',\n",
       " 'ADRO',\n",
       " 'INSY',\n",
       " 'ADVM',\n",
       " 'ADVM',\n",
       " 'NOK',\n",
       " 'VALE',\n",
       " 'NIO',\n",
       " 'ABEV',\n",
       " 'XHR',\n",
       " 'NYSE',\n",
       " 'CCBG',\n",
       " 'WWE',\n",
       " 'ADNT',\n",
       " 'BOOM',\n",
       " 'PCG',\n",
       " 'CYBR',\n",
       " 'CVNA',\n",
       " 'YETI',\n",
       " 'PUMP',\n",
       " 'RDN',\n",
       " 'NOK',\n",
       " 'VALE',\n",
       " 'NIO',\n",
       " 'ABEV',\n",
       " 'XHR',\n",
       " 'PEP',\n",
       " 'RLGT',\n",
       " 'DIS',\n",
       " 'SPY',\n",
       " 'WUHN',\n",
       " 'BOP',\n",
       " 'ESNC',\n",
       " 'WUHN',\n",
       " 'BOP',\n",
       " 'ESNC',\n",
       " 'WUHN',\n",
       " 'BOP',\n",
       " 'ESNC',\n",
       " 'WUHN',\n",
       " 'BOP',\n",
       " 'ESNC',\n",
       " 'WUHN',\n",
       " 'BOP',\n",
       " 'ESNC',\n",
       " 'WUHN',\n",
       " 'BOP',\n",
       " 'ESNC',\n",
       " 'WUHN',\n",
       " 'BOP',\n",
       " 'ESNC',\n",
       " 'WUHN',\n",
       " 'BOP',\n",
       " 'ESNC',\n",
       " 'GIB',\n",
       " 'GIB',\n",
       " 'FCX',\n",
       " 'FCX',\n",
       " 'NVA',\n",
       " 'NVA']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the tickers for period sign\n",
    "new_tickers=[]\n",
    "for t in extracted_tickers:\n",
    "    new_t=t.replace('.','')\n",
    "    new_tickers.append(new_t)\n",
    "new_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ESNC', 'WUHN', 'BOP', 'TSLA', 'AAPL', 'ROKU', 'BA', 'NOK', 'LYFT', 'PCG']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the list into dataframe to help sort\n",
    "df=pd.DataFrame({\"ticker\":new_tickers})\n",
    "\n",
    "# count ticker frequency and sort with descending order, take the top 10 tickers\n",
    "df1 = df.ticker.value_counts()\n",
    "top_ten_df=df1[0:10].reset_index()\n",
    "top_ten_ticker=list(top_ten_df['index'])\n",
    "\n",
    "# a display of our list of tickers\n",
    "top_ten_ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Yahoo Finance Scrape (using results from Part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up path\n",
    "executable_path = {\"executable_path\": \"chromedriver\"}\n",
    "browser = Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build query Url and List of Stocks ticker\n",
    "base_url = \"https://finance.yahoo.com/quote/\"\n",
    "tickers = top_ten_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and initialize list \n",
    "close_price_list = []\n",
    "volume_list = []\n",
    "mkt_cap_list = []\n",
    "earnings_Date_list = []\n",
    "eps_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6a75715c9168>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"W(100%)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Trsdu(0.3s) \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmarket_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"W(100%) M(0) Bdcl(c)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Trsdu(0.3s) \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#print(summary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#type(summary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    try:\n",
    "        # Visit Yahoo finance summary page\n",
    "        url1 = base_url + ticker + '?p=' + ticker + '&.tsrc=fin-srch'\n",
    "    #     print(url1)\n",
    "        browser.visit(url1)\n",
    "        # Scrape Financial Data into Soup\n",
    "        html = browser.html\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        summary = soup.find('table', class_=\"W(100%)\").find_all(class_=\"Trsdu(0.3s) \")\n",
    "        market_info = soup.find('table', class_=\"W(100%) M(0) Bdcl(c)\").find_all(class_=\"Trsdu(0.3s) \")\n",
    "        #print(summary)\n",
    "        #type(summary)\n",
    "        close_price = soup.find('div', class_=\"D(ib) Mend(20px)\").text\n",
    "    #     print(close_price.split('+')[0])\n",
    "        close_price = summary[0].contents[0]\n",
    "        close_price_list.append(close_price)\n",
    "        volume = summary[4].contents[0]\n",
    "        volume_list.append(volume)\n",
    "        market_cap = market_info[0].contents[0]\n",
    "        mkt_cap_list.append(market_cap)\n",
    "        earnings_Date = market_info[4].contents[0]\n",
    "        earnings_Date_list.append(earnings_Date)\n",
    "        eps = market_info[3].contents[0]\n",
    "        eps_list.append(eps)\n",
    "    \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'231,615'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove B and convert Mkt_cap to Float\n",
    "new_cap_list = []\n",
    "for cap in mkt_cap_list:\n",
    "    try:\n",
    "        new_cap = cap.replace('B', '')\n",
    "        new_cap = float(new_cap)\n",
    "        new_cap_list.append(new_cap)\n",
    "    except ValueError:\n",
    "        pass\n",
    "mkt_cap_list = new_cap_list\n",
    "print(mkt_cap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '449,380'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-322f58d8e13c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcap\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmkt_cap_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnew_cap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'B'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mnew_cap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_cap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mnew_cap_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_cap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmkt_cap_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_cap_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '449,380'"
     ]
    }
   ],
   "source": [
    "# remove comma from Volume values\n",
    "new_vol_list = []\n",
    "for vol in volume_list:\n",
    "    try:\n",
    "        new_vol = vol.replace(',', '')\n",
    "        new_vol = float(new_vol)\n",
    "        new_vol_list.append(new_vol)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "volume_list = new_vol_list\n",
    "# print(volume_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and initialize list \n",
    "company_name_list = []\n",
    "company_desc_list = []\n",
    "company_tel_list = []\n",
    "company_addr_list = []\n",
    "key_execs_n_list = []\n",
    "key_execs_t_list = []\n",
    "key_execs_s_list = []\n",
    "url_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    try:\n",
    "        # Visit Yahoo finance profile page\n",
    "        url2 = base_url + ticker + '/profile?p=' + ticker\n",
    "    #     print(url2)\n",
    "        browser.visit(url2)\n",
    "        # Scrape Financial Data into Soup\n",
    "        html = browser.html\n",
    "        soup = bs(html, \"html.parser\")\n",
    "\n",
    "        profile1 = soup.find('p', class_=\"D(ib) W(47.727%) Pend(40px)\")\n",
    "        company_addr = profile1.contents[1] + profile1.contents[5]\n",
    "        company_addr_list.append(company_addr)\n",
    "    #     print(company_addr)\n",
    "        #type(data)\n",
    "        company_name = soup.find(\"h3\", class_=\"Fz(m) Mb(10px)\").text\n",
    "        company_name_list.append(company_name)\n",
    "        company_desc = soup.find('p', class_=\"Mt(15px) Lh(1.6)\").text\n",
    "        company_desc_list.append(company_desc)\n",
    "    #     print(company_desc)\n",
    "        key_execs_data = soup.find('table', class_='W(100%)').find('tbody').find_all('span')\n",
    "#         print(key_execs_data[0])\n",
    "        key_execs_name = key_execs_data[0].contents[1]\n",
    "        key_execs_title = key_execs_data[1].contents[1]\n",
    "        key_execs_salary = key_execs_data[2].contents[1] \n",
    "        if len(key_execs_salary):\n",
    "            key_execs_salary = 'N/A'\n",
    "        key_execs_n_list.append(key_execs_name)\n",
    "        key_execs_t_list.append(key_execs_title)\n",
    "        key_execs_s_list.append(key_execs_salary)\n",
    "        profile = soup.find('p', class_=\"D(ib) W(47.727%) Pend(40px)\").find_all('a')\n",
    "        company_tel = profile[0].text\n",
    "        url = profile[1].text\n",
    "        company_tel_list.append(company_tel)\n",
    "        url_list.append(url)\n",
    "        \n",
    "    except IndexError as e:\n",
    "        pass\n",
    "    except AttributeError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert telephone number to Float\n",
    "new_tel_list = []\n",
    "for tel in company_tel_list:\n",
    "    try:\n",
    "        new_tel = tel.replace('-', '')\n",
    "        new_tel = float(new_tel)\n",
    "        new_tel_list.append(new_tel)\n",
    "    except ValueError:\n",
    "        pass\n",
    "company_tel_list = new_tel_list\n",
    "#print(company_tel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove M and convert Key_Execs_Salary to Float\n",
    "new_s_list = []\n",
    "for s in key_execs_s_list:\n",
    "    try:\n",
    "        new_s = s.replace('M', '')\n",
    "        new_s = float(new_s)\n",
    "        new_s_list.append(new_s)\n",
    "        \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "key_execs_s_list = new_s_list\n",
    "# print(key_execs_s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and initialize list \n",
    "total_ESG_score_list = []\n",
    "avg_perf_env_list = []\n",
    "social_score_list = []\n",
    "governance_score_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    try:\n",
    "        # Visit Yahoo finance sustanability page\n",
    "        url3 = base_url + ticker + '/sustainability?p=' + ticker\n",
    "    #     print(url3)\n",
    "        browser.visit(url3)\n",
    "        # Scrape Financial Data into Soup\n",
    "        html = browser.html\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        sustain = soup.find_all('div', class_=\"D(ib) Fz(23px) smartphone_Fz(22px) Fw(600)\")\n",
    "    #     print(sustain[count].contents[0])\n",
    "    #     count += 1\n",
    "        total_ESG_score = soup.find('div', class_=\"Fz(36px) Fw(600) D(ib) Mend(5px)\").text\n",
    "        total_ESG_score_list.append(total_ESG_score)\n",
    "        avg_perf_env = sustain[0].contents[0]\n",
    "        avg_perf_env_list.append(avg_perf_env)\n",
    "        social_score = sustain[1].contents[0]\n",
    "        social_score_list.append(social_score)\n",
    "        governance_score = sustain[2].contents[0]\n",
    "        governance_score_list.append(governance_score)\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Summary dataframe\n",
    "data2_dic = {'Ticker':tickers, 'Close_Price':close_price_list, 'Volume':volume_list,'Mkt_Cap':mkt_cap_list, 'EPS':eps_list, 'Earnings_Date':earnings_Date_list}\n",
    "summary_df = pd.DataFrame.from_dict(data2_dic, orient='index')\n",
    "summary_df = summary_df.transpose()\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Profile dataframe\n",
    "data_dic = {'Ticker':tickers, 'Company_Name':company_name_list, 'Description':company_desc_list, 'Telephone':company_tel_list, 'Address':company_addr_list,'Key_Execs_Name':key_execs_n_list,'Key_Execs_Title':key_execs_t_list,'Web_URL':url_list}\n",
    "profile_df = pd.DataFrame.from_dict(data_dic, orient='index')\n",
    "profile_df = profile_df.transpose()\n",
    "profile_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Combine Dataframe summary and profile\n",
    "profile_df = profile_df.drop(columns=['Ticker'])\n",
    "summary_profile_combine_df = pd.concat([profile_df, summary_df], axis=1, sort=True)\n",
    "summary_profile_combine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sustainability dataframe\n",
    "data1_dic = {'Ticker':tickers, 'Total_ESG_Score':total_ESG_score_list, 'AVG_Perf_Env':avg_perf_env_list,'Social_Score':social_score_list, 'Governance_Score':governance_score_list}\n",
    "sustainability_df = pd.DataFrame.from_dict(data1_dic, orient='index')\n",
    "sustainability_df = sustainability_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sustainability to combine dataframe\n",
    "sustainability_df = sustainability_df.drop(columns=['Ticker'])\n",
    "summary_profile_sustainability_df = pd.concat([summary_profile_combine_df, sustainability_df], axis=1, sort=True)\n",
    "summary_profile_sustainability_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading into Database Mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database \n",
    "rds_connection_string = \"root:root123@127.0.0.1:3306/twitter_stocks_db\"\n",
    "engine = create_engine(f'mysql://{rds_connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted Combine DataFrame into database table stock_data\n",
    "summary_profile_sustainability_df.to_sql(name='stock_data', con=engine, if_exists='replace', index=False)\n",
    "pd.read_sql_query('select * from stock_data', con=engine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted Summary DataFrame into database table summary\n",
    "summary_df.to_sql(name='summary', con=engine, if_exists='replace', index=False)\n",
    "pd.read_sql_query('select * from summary', con=engine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted profile DataFrame into database table profile\n",
    "# pd.to_sql('TRUNCATE TABLE profile', con=engine)\n",
    "profile_df.to_sql(name='profile', con=engine, if_exists='replace', index=False)\n",
    "pd.read_sql_query('select * from profile', con=engine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted sustainability DataFrame into database table sustainability\n",
    "sustainability_df.to_sql(name='sustainability', con=engine, if_exists='replace', index=False)\n",
    "pd.read_sql_query('select * from sustainability', con=engine).head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
